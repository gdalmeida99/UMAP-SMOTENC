{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbc8RdSh_MGN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import wilcoxon\n",
        "from scipy import stats\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "def aggregate_numeric(rank= False, remove_train=False):\n",
        "  \n",
        "  cols_new=[]\n",
        "  data0 = pd.read_csv(\"adult_st.csv\") \n",
        "  data1 = pd.read_csv(\"beans_st.csv\") \n",
        "  data2 = pd.read_csv(\"occupancy_st.csv\") \n",
        "  data3 = pd.read_csv(\"loan_st.csv\") \n",
        "  data4 = pd.read_csv(\"magic_st.csv\") \n",
        "  data5  = pd.read_csv(\"digits_st.csv\")\n",
        "  data6 = pd.read_csv(\"creditcard_st.csv\") \n",
        "  data7 = pd.read_csv(\"covertype_st.csv\") \n",
        "\n",
        "  cols = [ 'KSTest',\n",
        "          'Mean_Knn', '10_Knn', \n",
        "          'Mean_Ratio_Knn',  'Mean_Ratio_10Knn',\n",
        "        '10_Ratio_Knn', '10_Ratio_10Knn',\n",
        "          'phik_corr', \"mean_test_score\" ]\n",
        "  if remove_train==True:\n",
        "    data0 = data0[~data0[\"Generator\"].isin([\"train\"])]\n",
        "    data1 = data1[~data1[\"Generator\"].isin([\"train\"])]\n",
        "    data2 = data2[~data2[\"Generator\"].isin([\"train\"])]\n",
        "    data3 = data3[~data3[\"Generator\"].isin([\"train\"])]\n",
        "    data4 = data4[~data4[\"Generator\"].isin([\"train\"])]\n",
        "    data5 = data5[~data5[\"Generator\"].isin([\"train\"])]\n",
        "    data6 = data6[~data6[\"Generator\"].isin([\"train\"])]\n",
        "    data7 = data7[~data7[\"Generator\"].isin([\"train\"])]\n",
        "\n",
        "  data0.params = data0.params.astype(str)\n",
        "  data1.params = data1.params.astype(str)\n",
        "  data2.params = data2.params.astype(str)\n",
        "  data3.params = data3.params.astype(str)\n",
        "  data4.params = data4.params.astype(str)\n",
        "  data5.params = data5.params.astype(str)\n",
        "  data6.params = data6.params.astype(str)\n",
        "  data7.params = data7.params.astype(str)\n",
        "\n",
        "  t0 = data0.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index()\n",
        "  t1 = data1.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index()\n",
        "  t2 = data2.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index()\n",
        "  t3 = data3.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index()\n",
        "  t4 = data4.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index()\n",
        "  t5 = data5.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index()\n",
        "  t6 = data6.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index()\n",
        "  t7 = data7.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index()\n",
        "\n",
        "\n",
        "  u0 = data0.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index().groupby([\"Generator\", \"Model\", \"Seed\",\"fold\"])[cols].max().reset_index()\n",
        "  u1= data1.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index().groupby([\"Generator\", \"Model\", \"Seed\",\"fold\"])[cols].max().reset_index()\n",
        "  u2 = data2.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index().groupby([\"Generator\", \"Model\", \"Seed\",\"fold\"])[cols].max().reset_index()\n",
        "  u3 = data3.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index().groupby([\"Generator\", \"Model\", \"Seed\",\"fold\"])[cols].max().reset_index()\n",
        "  u4 = data4.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index().groupby([\"Generator\", \"Model\", \"Seed\",\"fold\"])[cols].max().reset_index()\n",
        "  u5 = data5.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index().groupby([\"Generator\", \"Model\", \"Seed\",\"fold\"])[cols].max().reset_index()\n",
        "  u6 = data6.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index().groupby([\"Generator\", \"Model\", \"Seed\",\"fold\"])[cols].max().reset_index()\n",
        "  u7 = data7.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index().groupby([\"Generator\", \"Model\", \"Seed\",\"fold\"])[cols].max().reset_index()\n",
        "\n",
        "  if rank == True:\n",
        "    cols_new=[]\n",
        "\n",
        "    for col in cols:\n",
        "      if col == \"phik_corr\":\n",
        "        asce=True\n",
        "      else:\n",
        "        asce = False\n",
        "      \n",
        "      u0[col+\"_Rank\"]= u0.groupby([\"Model\", \"Seed\", \"fold\"])[col].rank(ascending=asce)\n",
        "      u1[col+\"_Rank\"]= u1.groupby([\"Model\", \"Seed\", \"fold\"])[col].rank(ascending=asce)\n",
        "      u2[col+\"_Rank\"]= u2.groupby([\"Model\", \"Seed\", \"fold\"])[col].rank(ascending=asce)\n",
        "      u3[col+\"_Rank\"]= u3.groupby([\"Model\", \"Seed\", \"fold\"])[col].rank(ascending=asce)\n",
        "      u4[col+\"_Rank\"]= u4.groupby([\"Model\", \"Seed\", \"fold\"])[col].rank(ascending=asce)\n",
        "      u5[col+\"_Rank\"]= u5.groupby([\"Model\", \"Seed\", \"fold\"])[col].rank(ascending=asce)\n",
        "      u6[col+\"_Rank\"]= u6.groupby([\"Model\", \"Seed\", \"fold\"])[col].rank(ascending=asce)\n",
        "      u7[col+\"_Rank\"]= u7.groupby([\"Model\", \"Seed\", \"fold\"])[col].rank(ascending=asce)\n",
        "\n",
        "      cols_new.append(col+\"_Rank\")\n",
        "      \n",
        "    mean_a = t0.merge(u0, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_b = t1.merge(u1, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_c = t2.merge(u2, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_d = t3.merge(u3, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_e = t4.merge(u4, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_f = t5.merge(u5, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_g = t6.merge(u6, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_h = t7.merge(u7, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "\n",
        "\n",
        "    std_a = t0.merge(u0, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_b = t1.merge(u1, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_c = t2.merge(u2, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_d = t3.merge(u3, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_e = t4.merge(u4, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_f = t5.merge(u5, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_g = t6.merge(u6, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_h = t7.merge(u7, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "\n",
        "    std_a[cols_new] = std_a[cols_new]* std_a[cols_new]\n",
        "    std_b[cols_new] = std_b[cols_new]* std_b[cols_new]\n",
        "    std_c[cols_new] = std_c[cols_new]* std_c[cols_new]\n",
        "    std_d[cols_new] = std_d[cols_new]* std_d[cols_new]\n",
        "    std_e[cols_new] = std_e[cols_new]* std_e[cols_new]\n",
        "    std_f[cols_new] = std_f[cols_new]* std_f[cols_new]\n",
        "    std_g[cols_new] = std_g[cols_new]* std_g[cols_new]\n",
        "    std_h[cols_new] = std_h[cols_new]* std_h[cols_new]\n",
        "\n",
        "    std_a = pd.DataFrame(std_a.groupby([\"Generator\", \"Model\"]).mean()[cols_new].apply(lambda x: np.sqrt(x)))\n",
        "    std_b = pd.DataFrame(std_b.groupby([\"Generator\", \"Model\"]).mean()[cols_new].apply(lambda x: np.sqrt(x)))\n",
        "    std_c = pd.DataFrame(std_c.groupby([\"Generator\", \"Model\"]).mean()[cols_new].apply(lambda x: np.sqrt(x)))\n",
        "    std_d = pd.DataFrame(std_d.groupby([\"Generator\", \"Model\"]).mean()[cols_new].apply(lambda x: np.sqrt(x)))\n",
        "    std_e = pd.DataFrame(std_e.groupby([\"Generator\", \"Model\"]).mean()[cols_new].apply(lambda x: np.sqrt(x)))\n",
        "    std_f = pd.DataFrame(std_f.groupby([\"Generator\", \"Model\"]).mean()[cols_new].apply(lambda x: np.sqrt(x)))\n",
        "    std_g = pd.DataFrame(std_g.groupby([\"Generator\", \"Model\"]).mean()[cols_new].apply(lambda x: np.sqrt(x)))\n",
        "    std_h = pd.DataFrame(std_h.groupby([\"Generator\", \"Model\"]).mean()[cols_new].apply(lambda x: np.sqrt(x)))\n",
        "\n",
        "\n",
        "    std_a[cols_new] = std_a[cols_new]* std_a[cols_new]\n",
        "    std_b[cols_new] = std_b[cols_new]* std_b[cols_new]\n",
        "    std_c[cols_new] = std_c[cols_new]* std_c[cols_new]\n",
        "    std_d[cols_new] = std_d[cols_new]* std_d[cols_new]\n",
        "    std_e[cols_new] = std_e[cols_new]* std_e[cols_new]\n",
        "    std_f[cols_new] = std_f[cols_new]* std_f[cols_new]\n",
        "    std_g[cols_new] = std_g[cols_new]* std_g[cols_new]\n",
        "    std_h[cols_new] = std_h[cols_new]* std_h[cols_new]\n",
        "    \n",
        "    return cols, cols_new, [std_a,std_b,std_c,std_d,std_e,std_f,std_g, std_h], [mean_a,mean_b,mean_c, mean_d,mean_e,mean_f,mean_g, mean_h], [u0, u1, u2, u3, u4, u5, u6, u7]\n",
        "\n",
        "  else:\n",
        "    mean_a = t0.merge(u0, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_b = t1.merge(u1, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_c = t2.merge(u2, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_d = t3.merge(u3, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_e = t4.merge(u4, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_f = t5.merge(u5, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_g = t6.merge(u6, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_h = t7.merge(u7, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "\n",
        "\n",
        "    std_a = t0.merge(u0, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_b = t1.merge(u1, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_c = t2.merge(u2, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_d = t3.merge(u3, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_e = t4.merge(u4, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_f = t5.merge(u5, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_g = t6.merge(u6, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_h = t7.merge(u7, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "\n",
        "    std_a[cols] = std_a[cols]* std_a[cols]\n",
        "    std_b[cols] = std_b[cols]* std_b[cols]\n",
        "    std_c[cols] = std_c[cols]* std_c[cols]\n",
        "    std_d[cols] = std_d[cols]* std_d[cols]\n",
        "    std_e[cols] = std_e[cols]* std_e[cols]\n",
        "    std_f[cols] = std_f[cols]* std_f[cols]\n",
        "    std_g[cols] = std_g[cols]* std_g[cols]\n",
        "    std_h[cols] = std_h[cols]* std_h[cols]\n",
        "\n",
        "    std_a = pd.DataFrame(std_a.groupby([\"Generator\", \"Model\"]).mean()[cols].apply(lambda x: np.sqrt(x)))\n",
        "    std_b = pd.DataFrame(std_b.groupby([\"Generator\", \"Model\"]).mean()[cols].apply(lambda x: np.sqrt(x)))\n",
        "    std_c = pd.DataFrame(std_c.groupby([\"Generator\", \"Model\"]).mean()[cols].apply(lambda x: np.sqrt(x)))\n",
        "    std_d = pd.DataFrame(std_d.groupby([\"Generator\", \"Model\"]).mean()[cols].apply(lambda x: np.sqrt(x)))\n",
        "    std_e = pd.DataFrame(std_e.groupby([\"Generator\", \"Model\"]).mean()[cols].apply(lambda x: np.sqrt(x)))\n",
        "    std_f = pd.DataFrame(std_f.groupby([\"Generator\", \"Model\"]).mean()[cols].apply(lambda x: np.sqrt(x)))\n",
        "    std_g = pd.DataFrame(std_g.groupby([\"Generator\", \"Model\"]).mean()[cols].apply(lambda x: np.sqrt(x)))\n",
        "    std_h = pd.DataFrame(std_h.groupby([\"Generator\", \"Model\"]).mean()[cols].apply(lambda x: np.sqrt(x)))\n",
        "\n",
        "\n",
        "    std_a[cols] = std_a[cols]* std_a[cols]\n",
        "    std_b[cols] = std_b[cols]* std_b[cols]\n",
        "    std_c[cols] = std_c[cols]* std_c[cols]\n",
        "    std_d[cols] = std_d[cols]* std_d[cols]\n",
        "    std_e[cols] = std_e[cols]* std_e[cols]\n",
        "    std_f[cols] = std_f[cols]* std_f[cols]\n",
        "    std_g[cols] = std_g[cols]* std_g[cols]\n",
        "    std_h[cols] = std_h[cols]* std_h[cols]\n",
        "    \n",
        "    return cols, [std_a,std_b,std_c,std_d,std_e,std_f,std_g, std_h], [mean_a,mean_b,mean_c, mean_d,mean_e,mean_f,mean_g,mean_h],[u0, u1, u2, u3, u4, u5, u6, u7]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj6G1FGJazp4"
      },
      "outputs": [],
      "source": [
        "cols, stds, means, _ = aggregate_numeric()\n",
        "std_a,std_b,std_c,std_d,std_e,std_f,std_g,std_h = stds\n",
        "mean_a,mean_b,mean_c, mean_d,mean_e,mean_f,mean_g, mean_h= means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x72Q4PuqxFBj"
      },
      "outputs": [],
      "source": [
        "#Storing mean values per generator (and per classifier, when appropriate)\n",
        "for col in cols:\n",
        "  \n",
        "  stds=(std_a+std_b+std_c+std_d+std_e+std_f+std_g+std_h)/8\n",
        "  data = ((mean_a+mean_b+mean_c+mean_d+mean_e+mean_f+mean_g+mean_h)/8)[[col]]\n",
        "  data[\"std\"]= stds.apply(lambda x: np.sqrt(x))[[col]]\n",
        "  \n",
        "  if col !=  \"mean_test_score\":\n",
        "    data = data.groupby(\"Generator\").mean()\n",
        "  data.to_csv(col+\".csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un6XSXcdACQG",
        "outputId": "573cfbde-2841-4e17-9655-6d5270dcb6b2"
      },
      "outputs": [],
      "source": [
        "#The Wilcoxon signficance tests used\n",
        "cols, stds, means, _ = aggregate_numeric()\n",
        "std_a,std_b,std_c,std_d,std_e,std_f,std_g,std_h = stds\n",
        "mean_a,mean_b,mean_c, mean_d,mean_e,mean_f,mean_g, mean_h= means\n",
        "\n",
        "mean_a= mean_a.reset_index()\n",
        "mean_b= mean_b.reset_index()\n",
        "mean_c= mean_c.reset_index()\n",
        "mean_d= mean_d.reset_index()\n",
        "mean_e= mean_e.reset_index()\n",
        "mean_f= mean_f.reset_index()\n",
        "mean_g=mean_g.reset_index()\n",
        "mean_h=mean_h.reset_index()\n",
        "\n",
        "for col in cols:\n",
        "  cp =[]\n",
        "  gan =[]\n",
        "  umap_smote_nc =[]\n",
        "  smote =[]\n",
        "  vae = []\n",
        "  train =[]\n",
        "  p_values=[]\n",
        "\n",
        "  if col == \"mean_test_score\":\n",
        "    for classifier in [\"rf\",\"dt\", \"xgb\", \"mlp\", \"lr\",\"gs\"]:\n",
        "    \n",
        "      cp =[]\n",
        "      gan =[]\n",
        "      umap_smote_nc =[]\n",
        "      smote =[]\n",
        "      vae = []\n",
        "      train =[]\n",
        "      p_values=[]\n",
        "      for value in [mean_a,mean_b,mean_c,mean_d, mean_e, mean_f, mean_g,mean_h]:\n",
        "        data = value[value.Model==classifier]\n",
        "        [cp.append(x) for x in data[data.Generator == \"copula\"][col]]\n",
        "        [gan.append(x) for x in data[data.Generator == \"gan\"][col]]\n",
        "        [vae.append(x) for x in data[data.Generator == \"vae\"][col]]\n",
        "        [umap_smote_nc.append(x) for x in data[data.Generator == \"umap_smote_nc\"][col]]\n",
        "        [train.append(x) for x in data[data.Generator == \"train\"][col]]\n",
        "        [smote.append(x) for x in data[data.Generator == \"smote_nc\"][col]]\n",
        "\n",
        "      for i in [cp, gan, vae, smote]:\n",
        "        p_values.append(wilcoxon([t-j for (t,j) in zip(umap_smote_nc,i)])[1])\n",
        "\n",
        "      values=multipletests(p_values, method =\"holm\")[1]\n",
        "      [print(classifier, name, \"{:.1e}\".format(value)) for (value, name) in zip(values, [\"cp\", \"gan\", \"vae\", \"smote\"])]\n",
        "  \n",
        "  elif col in [\"Mean_Knn\", \"10_Knn\",\"Mean_Ratio_Knn\",\"Mean_Ratio_10Knn\", \"10_Ratio_Knn\", \"10_Ratio_10Knn\"]:\n",
        "    for value in [mean_a,mean_b,mean_c,mean_d, mean_e, mean_f, mean_g, mean_h]:\n",
        "      \n",
        "      data = value \n",
        "      [cp.append(x) for x in data[data.Generator == \"copula\"][col]]\n",
        "      [gan.append(x) for x in data[data.Generator == \"gan\"][col]]\n",
        "      [vae.append(x) for x in data[data.Generator == \"vae\"][col]]\n",
        "      [umap_smote_nc.append(x) for x in data[data.Generator == \"umap_smote_nc\"][col]]\n",
        "      [train.append(x) for x in data[data.Generator == \"train\"][col]]\n",
        "      [smote.append(x) for x in data[data.Generator == \"smote_nc\"][col]]\n",
        "\n",
        "    for i in [umap_smote_nc, smote]:\n",
        "      p_values.append(wilcoxon([t-j for (t,j) in zip(train,i)])[1])\n",
        "\n",
        "    values=multipletests(p_values, method =\"holm\")[1]\n",
        "    [print(col, name, \"{:.1e}\".format(value)) for (value, name) in zip(values, [\"umap_smote\", \"smote\"])]\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL2ymoiPoFgr"
      },
      "source": [
        "**Visual**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmgWb729TGHT"
      },
      "outputs": [],
      "source": [
        "cols, stds, means,_ = aggregate_numeric()\n",
        "std_a,std_b,std_c,std_d,std_e,std_f,std_g, std_h = stds\n",
        "mean_a,mean_b,mean_c, mean_d,mean_e,mean_f,mean_g, mean_h = means\n",
        "\n",
        "\n",
        "mean_a= mean_a.reset_index()\n",
        "mean_b= mean_b.reset_index()\n",
        "mean_c= mean_c.reset_index()\n",
        "mean_d= mean_d.reset_index()\n",
        "mean_e= mean_e.reset_index()\n",
        "mean_f= mean_f.reset_index()\n",
        "mean_g=mean_g.reset_index()\n",
        "mean_h=mean_h.reset_index()\n",
        "\n",
        "cp =[]\n",
        "gan =[]\n",
        "umap_smote_nc =[]\n",
        "smote =[]\n",
        "vae = []\n",
        "train =[]\n",
        "cp_dist =[]\n",
        "gan_dist =[]\n",
        "umap_smote_nc_dist =[]\n",
        "smote_dist =[]\n",
        "vae_dist = []\n",
        "\n",
        "for col in [\"mean_test_score\", \"Mean_Knn\"]:\n",
        "\n",
        "\n",
        "  if col == \"mean_test_score\":\n",
        "    for value in [mean_a,mean_b,mean_c,mean_d, mean_e, mean_f, mean_g, mean_h]:\n",
        "\n",
        "      for classifier in [\"rf\",\"dt\", \"xgb\", \"mlp\", \"lr\",\"gs\"]:\n",
        "        \n",
        "        data = value[value.Model==classifier].copy()\n",
        "        [cp.append(x) for x in data[data.Generator == \"copula\"][col]]\n",
        "        [gan.append(x) for x in data[data.Generator == \"gan\"][col]]\n",
        "        [vae.append(x) for x in data[data.Generator == \"vae\"][col]]\n",
        "        [umap_smote_nc.append(x) for x in data[data.Generator == \"umap_smote_nc\"][col]]\n",
        "        [smote.append(x) for x in data[data.Generator == \"smote_nc\"][col]]\n",
        "        [train.append(x) for x in data[data.Generator == \"train\"][col]]\n",
        "  else:\n",
        "    for value in [mean_a,mean_b,mean_c,mean_d, mean_e, mean_f, mean_g, mean_h]:\n",
        "      \n",
        "        data=value.reset_index().groupby(\"Generator\").mean().copy().reset_index()\n",
        "        [cp_dist.append(x) for x in data[data.Generator == \"copula\"][col]]\n",
        "        [gan_dist.append(x) for x in data[data.Generator == \"gan\"][col]]\n",
        "        [vae_dist.append(x) for x in data[data.Generator == \"vae\"][col]]\n",
        "        [umap_smote_nc_dist.append(x) for x in data[data.Generator == \"umap_smote_nc\"][col]]\n",
        "        [smote_dist.append(x) for x in data[data.Generator == \"smote_nc\"][col]]\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMeUUt64nJ43"
      },
      "outputs": [],
      "source": [
        "cp = pd.DataFrame(cp).groupby(np.arange(len(pd.DataFrame(cp))) // 6).mean()-pd.DataFrame(train).groupby(np.arange(len(pd.DataFrame(train))) // 6).mean()\n",
        "cp[\"Dist\"] = cp_dist\n",
        "cp.reset_index(inplace=True)\n",
        "cp.rename({\"index\": \"Dataset\", 0: \"Utility\", \"Dist\":\"Privacy\"}, axis=1, inplace=True)\n",
        "cp[\"Generator\"] = \"GC\"\n",
        "\n",
        "\n",
        "smote = pd.DataFrame(smote).groupby(np.arange(len(pd.DataFrame(smote))) // 6).mean()-pd.DataFrame(train).groupby(np.arange(len(pd.DataFrame(train))) // 6).mean()\n",
        "smote[\"Dist\"] = smote_dist\n",
        "smote.reset_index(inplace=True)\n",
        "smote.rename({\"index\": \"Dataset\", 0: \"Utility\", \"Dist\":\"Privacy\"}, axis=1, inplace=True)\n",
        "smote[\"Generator\"] = \"SMOTE-NC\"\n",
        "\n",
        "\n",
        "gan = pd.DataFrame(gan).groupby(np.arange(len(pd.DataFrame(gan))) // 6).mean()-pd.DataFrame(train).groupby(np.arange(len(pd.DataFrame(train))) // 6).mean()\n",
        "gan[\"Dist\"] = gan_dist\n",
        "gan.reset_index(inplace=True)\n",
        "gan.rename({\"index\": \"Dataset\", 0: \"Utility\", \"Dist\":\"Privacy\"}, axis=1, inplace=True)\n",
        "gan[\"Generator\"] = \"CTGAN\"\n",
        "\n",
        "vae = pd.DataFrame(vae).groupby(np.arange(len(pd.DataFrame(vae))) // 6).mean()-pd.DataFrame(train).groupby(np.arange(len(pd.DataFrame(train))) // 6).mean()\n",
        "vae[\"Dist\"] = vae_dist\n",
        "vae.reset_index(inplace=True)\n",
        "vae.rename({\"index\": \"Dataset\", 0: \"Utility\", \"Dist\":\"Privacy\"}, axis=1, inplace=True)\n",
        "vae[\"Generator\"] = \"TVAE\"\n",
        "\n",
        "umap_smote_nc = pd.DataFrame(umap_smote_nc).groupby(np.arange(len(pd.DataFrame(umap_smote_nc))) // 6).mean()-pd.DataFrame(train).groupby(np.arange(len(pd.DataFrame(train))) // 6).mean()\n",
        "umap_smote_nc[\"Dist\"] = umap_smote_nc_dist\n",
        "umap_smote_nc.reset_index(inplace=True)\n",
        "umap_smote_nc.rename({\"index\": \"Dataset\", 0: \"Utility\", \"Dist\":\"Privacy\"}, axis=1, inplace=True)\n",
        "umap_smote_nc[\"Generator\"] = \"UMAP-SMOTENC\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBjfg73aTvbM"
      },
      "outputs": [],
      "source": [
        "methods = umap_smote_nc.append(vae).append(gan).append(smote).append(cp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "YBYCLXDstFQa",
        "outputId": "97c2c32d-558f-42a2-8fa7-06fc48c45411"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt \n",
        "\n",
        "markers = [\"x\" , \">\" , \"o\" , \"v\"  , \",\"]\n",
        "colors = ['black','black','black','black','black']\n",
        "\n",
        "data = methods[methods[\"Dataset\"]==DATASET_NUMBER]\n",
        "\n",
        "\n",
        "\n",
        "for j, i in enumerate(data.Generator.unique()):  \n",
        "    mi = markers[j] \n",
        "    xi =  data[data.Generator == i][\"Privacy\"]\n",
        "    yi = data[data.Generator == i][\"Utility\"]  \n",
        "    ci = colors[j] \n",
        "    plt.scatter(xi,yi,marker=mi, color=ci)\n",
        "\n",
        "\n",
        "plt.xlim(data[\"Privacy\"].min()-0.05, data[\"Privacy\"].max()+0.05)\n",
        "plt.ylim(data[\"Utility\"].min()-0.1, data[\"Utility\"].max()+0.1)\n",
        "plt.legend(data.Generator.unique(),loc='upper center', bbox_to_anchor=(0.5, -0.25),\n",
        "          fancybox=True, shadow=True, ncol=5)\n",
        "\n",
        "\n",
        "plt.xlabel(\"Increase in Privacy\")\n",
        "plt.ylabel(\"Decrease in Utility\")\n",
        "ax = plt.gca()\n",
        "\n",
        "\n",
        "plt.savefig('DATASET_NAME.png',bbox_inches=\"tight\", dpi=300)\n",
        "\n",
        "plt.show() \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ5cdLrA2F4U"
      },
      "outputs": [],
      "source": [
        "  data0 = pd.read_csv(\"adult_st.csv\") \n",
        "  data1 = pd.read_csv(\"beans_st.csv\") \n",
        "  data2 = pd.read_csv(\"occupancy_st.csv\") \n",
        "  data3 = pd.read_csv(\"loan_st.csv\") \n",
        "  data4 = pd.read_csv(\"magic_st.csv\") \n",
        "  data5  = pd.read_csv(\"digits_st.csv\")\n",
        "  data6 = pd.read_csv(\"creditcard_st.csv\") \n",
        "  data7 = pd.read_csv(\"covertype_st.csv\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEDJyDq3HOy_"
      },
      "source": [
        "**Categorical**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqi1CTpJ-AYB"
      },
      "outputs": [],
      "source": [
        "def aggregate_cat(rank= False,remove_train=False):\n",
        "  \n",
        "  data3 = pd.read_csv(\"loan_st.csv\") \n",
        "  data5  = pd.read_csv(\"adult_st.csv\")\n",
        "  data7  = pd.read_csv(\"covertype_st.csv\")\n",
        "\n",
        "  if remove_train==True:\n",
        "\n",
        "    data3 = data3[~data3[\"Generator\"].isin([\"train\"])]\n",
        "    data5 = data5[~data5[\"Generator\"].isin([\"train\"])]\n",
        "    data7 = data7[~data7[\"Generator\"].isin([\"train\"])]\n",
        "\n",
        "  data3.params = data3.params.astype(str)\n",
        "  data5.params = data5.params.astype(str)\n",
        "  data7.params = data7.params.astype(str)\n",
        "\n",
        "  cols =[\"CSTest\"]\n",
        "\n",
        "  t3 = data3.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index()\n",
        "  t5 = data5.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index()\n",
        "  t7 = data7.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index()\n",
        "\n",
        "  u3 = data3.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index().groupby([\"Generator\", \"Model\", \"Seed\",\"fold\"])[cols].max().reset_index()\n",
        "  u5 = data5.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index().groupby([\"Generator\", \"Model\", \"Seed\",\"fold\"])[cols].max().reset_index()\n",
        "  u7 = data7.groupby([\"Generator\", \"Model\", \"Seed\", \"params\",\"fold\"])[cols].mean().reset_index().groupby([\"Generator\", \"Model\", \"Seed\",\"fold\"])[cols].max().reset_index()\n",
        " \n",
        "  if rank == True:\n",
        "    cols_new=[]\n",
        "\n",
        "    for col in cols:\n",
        "      if col == \"phik_corr\":\n",
        "        asce=True\n",
        "      else:\n",
        "        asce = False\n",
        "      \n",
        "\n",
        "      u3[col+\"_Rank\"]= u3.groupby([\"Model\", \"Seed\", \"fold\"])[col].rank(ascending=asce)\n",
        "      u5[col+\"_Rank\"]= u5.groupby([\"Model\", \"Seed\", \"fold\"])[col].rank(ascending=asce)\n",
        "      u7[col+\"_Rank\"]= u7.groupby([\"Model\", \"Seed\", \"fold\"])[col].rank(ascending=asce)\n",
        "\n",
        "      cols_new.append(col+\"_Rank\")\n",
        "      \n",
        "    mean_d = t3.merge(u3, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_f = t5.merge(u5, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_h = t7.merge(u7, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "\n",
        "    std_d = t3.merge(u3, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_f = t5.merge(u5, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_h = t7.merge(u7, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols_new].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "\n",
        "    std_d[cols_new] = std_d[cols_new]* std_d[cols_new]\n",
        "    std_f[cols_new] = std_f[cols_new]* std_f[cols_new]\n",
        "    std_h[cols_new] = std_h[cols_new]* std_h[cols_new]\n",
        "\n",
        "    std_d = pd.DataFrame(std_d.groupby([\"Generator\", \"Model\"]).mean()[cols_new].apply(lambda x: np.sqrt(x)))\n",
        "    std_f = pd.DataFrame(std_f.groupby([\"Generator\", \"Model\"]).mean()[cols_new].apply(lambda x: np.sqrt(x)))\n",
        "    std_h = pd.DataFrame(std_h.groupby([\"Generator\", \"Model\"]).mean()[cols_new].apply(lambda x: np.sqrt(x)))\n",
        "\n",
        "    std_d[cols_new] = std_d[cols_new]* std_d[cols_new]\n",
        "    std_f[cols_new] = std_f[cols_new]* std_f[cols_new]\n",
        "    std_h[cols_new] = std_h[cols_new]* std_h[cols_new]\n",
        " \n",
        "    return cols, cols_new, [std_d,std_f, std_h], [mean_d,mean_f, mean_h], [u3, u5, u7]\n",
        "  \n",
        "  else:\n",
        "    mean_d = t3.merge(u3, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_f = t5.merge(u5, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "    mean_h = t7.merge(u7, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\"]).mean()\n",
        "\n",
        "    std_d = t3.merge(u3, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_f = t5.merge(u5, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "    std_h = t7.merge(u7, on = [\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols)[[\"Generator\", \"Model\", \"Seed\",\"fold\"]+cols].drop_duplicates().groupby([\"Generator\", \"Model\", \"Seed\"]).std()\n",
        "\n",
        "    std_d[cols] = std_d[cols]* std_d[cols]\n",
        "    std_f[cols] = std_f[cols]* std_f[cols]\n",
        "    std_h[cols] = std_h[cols]* std_h[cols]\n",
        "\n",
        "    std_d = pd.DataFrame(std_d.groupby([\"Generator\", \"Model\"]).mean()[cols].apply(lambda x: np.sqrt(x)))\n",
        "    std_f = pd.DataFrame(std_f.groupby([\"Generator\", \"Model\"]).mean()[cols].apply(lambda x: np.sqrt(x)))\n",
        "    std_h = pd.DataFrame(std_h.groupby([\"Generator\", \"Model\"]).mean()[cols].apply(lambda x: np.sqrt(x)))\n",
        "\n",
        "    std_d[cols] = std_d[cols]* std_d[cols]\n",
        "    std_f[cols] = std_f[cols]* std_f[cols]\n",
        "    std_h[cols] = std_h[cols]* std_h[cols]\n",
        "\n",
        "    return cols, [std_d,std_f, std_h], [mean_d,mean_f, mean_h], [u3, u5, u7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75jGCogUavci"
      },
      "outputs": [],
      "source": [
        "cols, stds, means, _ = aggregate_cat()\n",
        "std_d,std_f, std_h= stds\n",
        "mean_d, mean_f, mean_h= means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSc0GTIuwxaV"
      },
      "outputs": [],
      "source": [
        "for col in cols:\n",
        "  \n",
        "  stds=(std_d+std_f+std_h)/3\n",
        "  data = ((mean_d+mean_f+mean_h)/3)[[col]]\n",
        "  data[\"std\"]= stds.apply(lambda x: np.sqrt(x))[[col]]\n",
        "  \n",
        "  data = data.groupby(\"Generator\").mean()\n",
        "  data.to_csv(col+\".csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0egjf95HId8"
      },
      "source": [
        "**Ranks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY_2EEkZdUDU"
      },
      "outputs": [],
      "source": [
        "cols, cols_new, stds, means, us = aggregate_numeric(rank=True, remove_train=True)\n",
        "std_a,std_b,std_c,std_d,std_e,std_f,std_g, std_h = stds\n",
        "mean_a,mean_b,mean_c, mean_d,mean_e,mean_f,mean_g,mean_h= means\n",
        "u0,u1,u2, u3,u4,u5,u6, u7 = us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-kahAZBMFGD"
      },
      "outputs": [],
      "source": [
        "for col in cols:\n",
        "  \n",
        "  stds=(std_a+std_b+std_c+std_d+std_e+std_f+std_g+std_h)/8\n",
        "  data = ((mean_a+mean_b+mean_c+mean_d+mean_e+mean_f+mean_g+mean_h)/8)[[col]]\n",
        "  data[\"std\"]= stds.apply(lambda x: np.sqrt(x))[[col]]\n",
        "  data.reset_index(inplace=True)\n",
        "  data = data[~data.Generator.isin([\"train\"])]\n",
        "  if col !=  \"mean_test_score_Rank\":\n",
        "    \n",
        "    data = data.groupby(\"Generator\").mean()\n",
        "  data.to_csv(col+\".csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDxVtKfKgb7I"
      },
      "source": [
        "**Friedman Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq9MPxyiL-v0",
        "outputId": "1a595613-9698-4fb8-e0aa-f4b5140f7a75"
      },
      "outputs": [],
      "source": [
        "for col in cols_new:\n",
        "  cp = []\n",
        "  gan = []\n",
        "  vae = []\n",
        "  umap_smote_nc = []\n",
        "  smote=[]\n",
        "  train =[]\n",
        "  if col == \"mean_test_score_Rank\": \n",
        "\n",
        "   \n",
        "    for model in [\"gs\",\"rf\", \"xgb\", \"lr\", \"mlp\", \"dt\"]:\n",
        "      cp = []\n",
        "      gan = []\n",
        "      vae = []\n",
        "      umap_smote_nc = []\n",
        "      smote=[]\n",
        "      train =[]\n",
        "      for i in [u0, u1, u2, u3, u4, u5, u6]:\n",
        "        data = i.groupby([\"Generator\", \"Model\"])[[col]].mean().reset_index()\n",
        "        data = data[data[\"Model\"]==model]\n",
        "        [cp.append(x) for x in data[data.Generator == \"copula\"][col]]\n",
        "        [gan.append(x) for x in data[data.Generator == \"gan\"][col]]\n",
        "        [vae.append(x) for x in data[data.Generator == \"vae\"][col]]\n",
        "        [umap_smote_nc.append(x) for x in data[data.Generator == \"umap_smote_nc\"][col]]\n",
        "        [smote.append(x) for x in data[data.Generator == \"smote_nc\"][col]]\n",
        "        [train.append(x) for x in data[data.Generator == \"train\"][col]]\n",
        "\n",
        "      _, p_value = stats.friedmanchisquare(cp, gan,vae, umap_smote_nc, smote)\n",
        "      print(\"{:.1e}\".format(p_value), col, model)\n",
        "      if p_value >=0.05:\n",
        "        print(p_value, col, model)\n",
        "  else:\n",
        "\n",
        "    for i in [u0, u1, u2, u3, u4, u5, u6]:\n",
        "      data = i.groupby([\"Generator\"])[[col]].mean().reset_index()\n",
        "      [cp.append(x) for x in data[data.Generator == \"copula\"][col]]\n",
        "      [gan.append(x) for x in data[data.Generator == \"gan\"][col]]\n",
        "      [vae.append(x) for x in data[data.Generator == \"vae\"][col]]\n",
        "      [smote.append(x) for x in data[data.Generator == \"smote_nc\"][col]]\n",
        "      [train.append(x) for x in data[data.Generator == \"train\"][col]]\n",
        "      [umap_smote_nc.append(x) for x in data[data.Generator == \"umap_smote_nc\"][col]]\n",
        "\n",
        "    \n",
        "    _, p_value = stats.friedmanchisquare(cp, gan, vae, umap_smote_nc, smote)\n",
        "    print(\"{:.1e}\".format(p_value), col)\n",
        "    if p_value >=0.05:\n",
        "      print(p_value, col)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk14ipSGkoVc"
      },
      "source": [
        "**Rank Categorical**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeb8vepPmAO5"
      },
      "outputs": [],
      "source": [
        "cols, cols_new, stds, means, us = aggregate_cat(rank=True, remove_train=True)\n",
        "std_d,std_f,std_h = stds\n",
        "mean_d, mean_f , mean_h= means\n",
        "u3, u5, u7 = us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf16cKaeZ4QR"
      },
      "outputs": [],
      "source": [
        "for col in cols_new:\n",
        "  \n",
        "  stds=(std_d+std_f+std_h)/3\n",
        "  data = ((mean_d+mean_f+mean_h)/3)[[col]]\n",
        "  data[\"std\"]= stds.apply(lambda x: np.sqrt(x))[[col]]\n",
        "  data.reset_index(inplace=True)\n",
        "  data = data.groupby(\"Generator\").mean()\n",
        "  data.to_csv(col+\".csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6pTw_RHmcXG"
      },
      "source": [
        "**Friedman Test Categorical**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DpDJBYdYhXL",
        "outputId": "1b566688-d1b4-468d-8f30-524699d2447a"
      },
      "outputs": [],
      "source": [
        "for col in cols_new:\n",
        "  cp = []\n",
        "  gan = []\n",
        "  vae = []\n",
        "  umap_smote_nc = []\n",
        "  smote=[]\n",
        "  train =[]\n",
        "\n",
        "\n",
        "  for i in [u3, u5, u7]:\n",
        "    data = i.groupby([\"Generator\"])[[col]].mean().reset_index()\n",
        "    [cp.append(x) for x in data[data.Generator == \"copula\"][col]]\n",
        "    [gan.append(x) for x in data[data.Generator == \"gan\"][col]]\n",
        "    [vae.append(x) for x in data[data.Generator == \"vae\"][col]]\n",
        "    [smote.append(x) for x in data[data.Generator == \"smote_nc\"][col]]\n",
        "    [umap_smote_nc.append(x) for x in data[data.Generator == \"umap_smote_nc\"][col]]\n",
        "\n",
        "    \n",
        "  _, p_value = stats.friedmanchisquare(cp, gan, vae, umap_smote_nc, smote)\n",
        "  print(\"{:.1e}\".format(p_value), col)\n",
        "  if p_value >=0.05:\n",
        "    print(p_value, col)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
